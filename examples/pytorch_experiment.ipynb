{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trixi PyTorch Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch multi processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from trixi.util import Config\n",
    "from trixi.experiment import PytorchExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180622-165237_experiment\t   data\r\n",
      "20180622-165352_resume_experiment  test-experiment\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf experiment_dir/20*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106M\texperiment_dir/\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh experiment_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Config()\n",
    "\n",
    "c.batch_size = 64\n",
    "c.batch_size_test = 1000\n",
    "c.n_epochs = 10\n",
    "c.learning_rate = 0.01\n",
    "c.momentum = 0.9\n",
    "if torch.cuda.is_available():\n",
    "    c.use_cuda = True\n",
    "else:\n",
    "    c.use_cuda = False\n",
    "c.data_loader_kwargs = {'num_workers': 1, 'pin_memory': True} if c.use_cuda else {}\n",
    "c.rnd_seed = 1\n",
    "c.log_interval = 200\n",
    "\n",
    "c.train_loader = {\n",
    "    torch.utils.data.DataLoader: {\n",
    "        'dataset': {\n",
    "            datasets.MNIST: {\n",
    "                'root': 'experiment_dir/data/',\n",
    "                'train': True,\n",
    "                'download': True,\n",
    "                'transform': {\n",
    "                    transforms.ToTensor: {}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'batch_size': c.batch_size,\n",
    "        **c.data_loader_kwargs\n",
    "    }\n",
    "}\n",
    "\n",
    "c.test_loader = {\n",
    "    torch.utils.data.DataLoader: {\n",
    "        'dataset': {\n",
    "            datasets.MNIST: {\n",
    "                'root': 'experiment_dir/data/',\n",
    "                'train': False,\n",
    "                'download': True,\n",
    "                'transform': {\n",
    "                    transforms.ToTensor: {}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'batch_size': c.batch_size_test,\n",
    "        **c.data_loader_kwargs\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple cnn model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_experiment(PytorchExperiment):\n",
    "    def setup(self):\n",
    "        self.train_data_loader = self.config.train_loader\n",
    "        self.test_data_loader = self.config.test_loader\n",
    "        self.model = Net()\n",
    "        if self.config.use_cuda:\n",
    "            self.model.cuda()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.config.learning_rate,\n",
    "                                               momentum=self.config.momentum)\n",
    "        self.save_checkpoint(name=\"checkpoint_start\")\n",
    "        self.vlog.plot_model_structure(self.model,\n",
    "                                       [self.config.batch_size, 1, 28, 28], \n",
    "                                       name='Model Structure')\n",
    "        self.elog.print('Experiment set up.')\n",
    "        self.batch_counter = 0\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.model.train()\n",
    "        for batch_idx, (data, target) in enumerate(self.train_data_loader):\n",
    "            self.batch_counter += 1\n",
    "            if self.config.use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            self.loss = F.nll_loss(output, target)\n",
    "            self.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % self.config.log_interval == 0:\n",
    "                # plot train loss\n",
    "                self.vlog.show_value(value=self.loss.item(), name='Loss',\n",
    "                                     count=self.batch_counter, tag='Train Loss')\n",
    "                # log train batch loss and progress\n",
    "                self.elog.print(\n",
    "                    'Train Epoch: {} [{}/{} samples ({:.0f}%)]\\t Batch Loss: {:.6f}'\n",
    "                    .format(epoch, batch_idx * len(data),\n",
    "                            len(self.train_data_loader.dataset),\n",
    "                            100. * batch_idx / len(self.train_data_loader),\n",
    "                            self.loss.item()))\n",
    "                self.save_checkpoint(name=\"checkpoint\", n_iter=batch_idx)\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        validation_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in self.test_data_loader:\n",
    "            if self.config.use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = self.model(data)\n",
    "            validation_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        validation_loss /= len(self.test_data_loader.dataset)\n",
    "        # plot the test loss\n",
    "        self.vlog.show_value(value=validation_loss, name='Loss',\n",
    "                             count=self.batch_counter, tag='Validation Loss')\n",
    "        # log validation loss and accuracy\n",
    "        self.elog.print(\n",
    "            '\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "            .format(validation_loss, correct, len(self.test_data_loader.dataset),\n",
    "                    100. * correct / len(self.test_data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp = MNIST_experiment(config=c, name='experiment', n_epochs=c.n_epochs, \n",
    "                       seed=42, base_dir='./experiment_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment set up.\n",
      "Experiment started.\n",
      "Train Epoch: 0 [0/60000 samples (0%)]\t Batch Loss: 2.316128\n",
      "Train Epoch: 0 [12800/60000 samples (21%)]\t Batch Loss: 0.823947\n",
      "Train Epoch: 0 [25600/60000 samples (43%)]\t Batch Loss: 0.590966\n",
      "Train Epoch: 0 [38400/60000 samples (64%)]\t Batch Loss: 0.366280\n",
      "Train Epoch: 0 [51200/60000 samples (85%)]\t Batch Loss: 0.551475\n",
      "\n",
      "Validation set: Average loss: 0.1424, Accuracy: 9568/10000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 samples (0%)]\t Batch Loss: 0.323969\n",
      "Train Epoch: 1 [12800/60000 samples (21%)]\t Batch Loss: 0.222087\n",
      "Train Epoch: 1 [25600/60000 samples (43%)]\t Batch Loss: 0.327640\n",
      "Train Epoch: 1 [38400/60000 samples (64%)]\t Batch Loss: 0.303253\n",
      "Train Epoch: 1 [51200/60000 samples (85%)]\t Batch Loss: 0.267672\n",
      "\n",
      "Validation set: Average loss: 0.0941, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 samples (0%)]\t Batch Loss: 0.110320\n",
      "Train Epoch: 2 [12800/60000 samples (21%)]\t Batch Loss: 0.271405\n",
      "Train Epoch: 2 [25600/60000 samples (43%)]\t Batch Loss: 0.265525\n",
      "Train Epoch: 2 [38400/60000 samples (64%)]\t Batch Loss: 0.220669\n",
      "Train Epoch: 2 [51200/60000 samples (85%)]\t Batch Loss: 0.296196\n",
      "\n",
      "Validation set: Average loss: 0.0731, Accuracy: 9772/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 samples (0%)]\t Batch Loss: 0.187863\n",
      "Train Epoch: 3 [12800/60000 samples (21%)]\t Batch Loss: 0.249179\n",
      "Train Epoch: 3 [25600/60000 samples (43%)]\t Batch Loss: 0.249765\n",
      "Train Epoch: 3 [38400/60000 samples (64%)]\t Batch Loss: 0.175101\n",
      "Train Epoch: 3 [51200/60000 samples (85%)]\t Batch Loss: 0.205843\n",
      "\n",
      "Validation set: Average loss: 0.0646, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 samples (0%)]\t Batch Loss: 0.133275\n",
      "Train Epoch: 4 [12800/60000 samples (21%)]\t Batch Loss: 0.252842\n",
      "Train Epoch: 4 [25600/60000 samples (43%)]\t Batch Loss: 0.396217\n",
      "Train Epoch: 4 [38400/60000 samples (64%)]\t Batch Loss: 0.187088\n",
      "Train Epoch: 4 [51200/60000 samples (85%)]\t Batch Loss: 0.269510\n",
      "\n",
      "Validation set: Average loss: 0.0581, Accuracy: 9819/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 samples (0%)]\t Batch Loss: 0.093963\n",
      "Train Epoch: 5 [12800/60000 samples (21%)]\t Batch Loss: 0.315611\n",
      "Train Epoch: 5 [25600/60000 samples (43%)]\t Batch Loss: 0.045552\n",
      "Train Epoch: 5 [38400/60000 samples (64%)]\t Batch Loss: 0.081655\n",
      "Train Epoch: 5 [51200/60000 samples (85%)]\t Batch Loss: 0.446825\n",
      "\n",
      "Validation set: Average loss: 0.0541, Accuracy: 9829/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 samples (0%)]\t Batch Loss: 0.143570\n",
      "Train Epoch: 6 [12800/60000 samples (21%)]\t Batch Loss: 0.140202\n",
      "Train Epoch: 6 [25600/60000 samples (43%)]\t Batch Loss: 0.081847\n",
      "Train Epoch: 6 [38400/60000 samples (64%)]\t Batch Loss: 0.196494\n",
      "Train Epoch: 6 [51200/60000 samples (85%)]\t Batch Loss: 0.553103\n",
      "\n",
      "Validation set: Average loss: 0.0499, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 samples (0%)]\t Batch Loss: 0.161484\n",
      "Train Epoch: 7 [12800/60000 samples (21%)]\t Batch Loss: 0.171787\n",
      "Train Epoch: 7 [25600/60000 samples (43%)]\t Batch Loss: 0.108846\n",
      "Train Epoch: 7 [38400/60000 samples (64%)]\t Batch Loss: 0.150505\n",
      "Train Epoch: 7 [51200/60000 samples (85%)]\t Batch Loss: 0.265023\n",
      "\n",
      "Validation set: Average loss: 0.0472, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 samples (0%)]\t Batch Loss: 0.152056\n",
      "Train Epoch: 8 [12800/60000 samples (21%)]\t Batch Loss: 0.126589\n",
      "Train Epoch: 8 [25600/60000 samples (43%)]\t Batch Loss: 0.058926\n",
      "Train Epoch: 8 [38400/60000 samples (64%)]\t Batch Loss: 0.166061\n",
      "Train Epoch: 8 [51200/60000 samples (85%)]\t Batch Loss: 0.385727\n",
      "\n",
      "Validation set: Average loss: 0.0450, Accuracy: 9850/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 samples (0%)]\t Batch Loss: 0.123342\n",
      "Train Epoch: 9 [12800/60000 samples (21%)]\t Batch Loss: 0.179335\n",
      "Train Epoch: 9 [25600/60000 samples (43%)]\t Batch Loss: 0.053074\n",
      "Train Epoch: 9 [38400/60000 samples (64%)]\t Batch Loss: 0.137082\n",
      "Train Epoch: 9 [51200/60000 samples (85%)]\t Batch Loss: 0.192288\n",
      "\n",
      "Validation set: Average loss: 0.0422, Accuracy: 9872/10000 (98%)\n",
      "\n",
      "Training complete.\n",
      "Experiment ended. Checkpoints stored =)\n",
      "Experiment ended.\n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "last_experiment = 'experiment_dir/' + sorted([d for d in os.listdir('experiment_dir/') if '20' in str(d)], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180622-165631_experiment  data  test-experiment\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment_dir/20180622-165631_experiment'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: Zugriff auf 'experiment_dir/20180622-164650_experiment/checkpoint/' nicht möglich: Datei oder Verzeichnis nicht gefunden\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir/20180622-164650_experiment/checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trixi.experiment import PytorchExperiment\n",
    "exp_resume = MNIST_experiment(config=c, name='resume_experiment', \n",
    "                              n_epochs=c.n_epochs, seed=42, base_dir='./experiment_dir', \n",
    "                              resume=last_experiment, resume_save_types=('model',\n",
    "                                                                         'simple',\n",
    "                                                                         'th_vars',\n",
    "                                                                         'results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment set up.\n",
      "Loaded existing config from: experiment_dir/20180622-165631_experiment\n",
      "Loaded existing checkpoint from: experiment_dir/20180622-165631_experiment/checkpoint/checkpoint_last.pth.tar\n",
      "Experiment started.\n",
      "Train Epoch: 0 [0/60000 samples (0%)]\t Batch Loss: 0.087793\n",
      "Train Epoch: 0 [12800/60000 samples (21%)]\t Batch Loss: 0.059094\n",
      "Train Epoch: 0 [25600/60000 samples (43%)]\t Batch Loss: 0.089623\n",
      "Train Epoch: 0 [38400/60000 samples (64%)]\t Batch Loss: 0.077594\n",
      "Train Epoch: 0 [51200/60000 samples (85%)]\t Batch Loss: 0.563244\n",
      "\n",
      "Validation set: Average loss: 0.0450, Accuracy: 9862/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 samples (0%)]\t Batch Loss: 0.046752\n",
      "Train Epoch: 1 [12800/60000 samples (21%)]\t Batch Loss: 0.148246\n",
      "Train Epoch: 1 [25600/60000 samples (43%)]\t Batch Loss: 0.111688\n",
      "Train Epoch: 1 [38400/60000 samples (64%)]\t Batch Loss: 0.163016\n",
      "Train Epoch: 1 [51200/60000 samples (85%)]\t Batch Loss: 0.184170\n",
      "\n",
      "Validation set: Average loss: 0.0413, Accuracy: 9870/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 samples (0%)]\t Batch Loss: 0.080354\n",
      "Train Epoch: 2 [12800/60000 samples (21%)]\t Batch Loss: 0.276413\n",
      "Train Epoch: 2 [25600/60000 samples (43%)]\t Batch Loss: 0.062202\n",
      "Train Epoch: 2 [38400/60000 samples (64%)]\t Batch Loss: 0.169774\n",
      "Train Epoch: 2 [51200/60000 samples (85%)]\t Batch Loss: 0.123602\n",
      "\n",
      "Validation set: Average loss: 0.0392, Accuracy: 9870/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 samples (0%)]\t Batch Loss: 0.112270\n",
      "Train Epoch: 3 [12800/60000 samples (21%)]\t Batch Loss: 0.206031\n",
      "Train Epoch: 3 [25600/60000 samples (43%)]\t Batch Loss: 0.112326\n",
      "Train Epoch: 3 [38400/60000 samples (64%)]\t Batch Loss: 0.115092\n",
      "Train Epoch: 3 [51200/60000 samples (85%)]\t Batch Loss: 0.188570\n",
      "\n",
      "Validation set: Average loss: 0.0374, Accuracy: 9874/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 samples (0%)]\t Batch Loss: 0.142928\n",
      "Train Epoch: 4 [12800/60000 samples (21%)]\t Batch Loss: 0.207236\n",
      "Train Epoch: 4 [25600/60000 samples (43%)]\t Batch Loss: 0.126559\n",
      "Train Epoch: 4 [38400/60000 samples (64%)]\t Batch Loss: 0.143978\n",
      "Train Epoch: 4 [51200/60000 samples (85%)]\t Batch Loss: 0.206036\n",
      "\n",
      "Validation set: Average loss: 0.0405, Accuracy: 9887/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 samples (0%)]\t Batch Loss: 0.037845\n",
      "Train Epoch: 5 [12800/60000 samples (21%)]\t Batch Loss: 0.138616\n",
      "Train Epoch: 5 [25600/60000 samples (43%)]\t Batch Loss: 0.035644\n",
      "Train Epoch: 5 [38400/60000 samples (64%)]\t Batch Loss: 0.072036\n",
      "Train Epoch: 5 [51200/60000 samples (85%)]\t Batch Loss: 0.433651\n",
      "\n",
      "Validation set: Average loss: 0.0379, Accuracy: 9878/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 samples (0%)]\t Batch Loss: 0.039524\n",
      "Train Epoch: 6 [12800/60000 samples (21%)]\t Batch Loss: 0.185545\n",
      "Train Epoch: 6 [25600/60000 samples (43%)]\t Batch Loss: 0.072087\n",
      "Train Epoch: 6 [38400/60000 samples (64%)]\t Batch Loss: 0.160648\n",
      "Train Epoch: 6 [51200/60000 samples (85%)]\t Batch Loss: 0.517118\n",
      "\n",
      "Validation set: Average loss: 0.0344, Accuracy: 9892/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 samples (0%)]\t Batch Loss: 0.151601\n",
      "Train Epoch: 7 [12800/60000 samples (21%)]\t Batch Loss: 0.127547\n",
      "Train Epoch: 7 [25600/60000 samples (43%)]\t Batch Loss: 0.043462\n",
      "Train Epoch: 7 [38400/60000 samples (64%)]\t Batch Loss: 0.142261\n",
      "Train Epoch: 7 [51200/60000 samples (85%)]\t Batch Loss: 0.208747\n",
      "\n",
      "Validation set: Average loss: 0.0369, Accuracy: 9884/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 samples (0%)]\t Batch Loss: 0.086481\n",
      "Train Epoch: 8 [12800/60000 samples (21%)]\t Batch Loss: 0.126209\n",
      "Train Epoch: 8 [25600/60000 samples (43%)]\t Batch Loss: 0.078929\n",
      "Train Epoch: 8 [38400/60000 samples (64%)]\t Batch Loss: 0.065847\n",
      "Train Epoch: 8 [51200/60000 samples (85%)]\t Batch Loss: 0.404844\n",
      "\n",
      "Validation set: Average loss: 0.0368, Accuracy: 9889/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 samples (0%)]\t Batch Loss: 0.066540\n",
      "Train Epoch: 9 [12800/60000 samples (21%)]\t Batch Loss: 0.195189\n",
      "Train Epoch: 9 [25600/60000 samples (43%)]\t Batch Loss: 0.079096\n",
      "Train Epoch: 9 [38400/60000 samples (64%)]\t Batch Loss: 0.134765\n",
      "Train Epoch: 9 [51200/60000 samples (85%)]\t Batch Loss: 0.106655\n",
      "\n",
      "Validation set: Average loss: 0.0415, Accuracy: 9888/10000 (98%)\n",
      "\n",
      "Training complete.\n",
      "Experiment ended. Checkpoints stored =)\n",
      "Experiment ended.\n"
     ]
    }
   ],
   "source": [
    "exp_resume.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180622-165631_experiment\t   data\r\n",
      "20180622-165742_resume_experiment  test-experiment\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
