{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trixi PyTorch Example\n",
    "\n",
    "This example aims to show some basic logging/visualization features for a simple workflow in PyTorch.  \n",
    "It assumes you have a visdom server running (using port 8080, so you can in principle even connect to the visdom server from some remote device):  \n",
    "`python -m visdom.server -port 8080`  \n",
    "Navigate your browser to `localhost:8080` to view the visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trixi import PytorchExperimentLogger\n",
    "from trixi import PytorchVisdomLogger\n",
    "from trixi import Config\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a `PytorchExperimentLogger` and a `PytorchVisdomLogger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = PytorchExperimentLogger(base_dir=\"./experiment_dir\", \n",
    "                              experiment_name=\"test-experiment\",\n",
    "                              folder_format=\"{experiment_name}\")\n",
    "\n",
    "Viz = PytorchVisdomLogger(name=\"main\", port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment logger creates a convenient folder structure for a given experiment (with name `experiment_name`) in the base directory `base_dir`. To better keep track of when a given experiment was done, `folder_format` allows using [Python's strftime directives](http://strftime.org/). An example folder format using the current date would be:  \n",
    "`folder_format=\"%Y-%m-%d_%H-%M_{experiment_name}\"`\n",
    "\n",
    "For the sake of this basic example we'll stick to the simple folder format only using the experiment's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-experiment\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the base directory has been created containing our test experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  config  img  log  plot  result  save\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir/test-experiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a config for our experiment which basically works like a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = Config()\n",
    "\n",
    "config_dict.batch_size = 64\n",
    "config_dict.batch_size_test = 1000\n",
    "config_dict.n_epochs = 10\n",
    "config_dict.learning_rate = 0.01\n",
    "config_dict.momentum = 0.9\n",
    "if torch.cuda.is_available():\n",
    "    config_dict.use_cuda = True\n",
    "else:\n",
    "    config_dict.use_cuda = False\n",
    "config_dict.rnd_seed = 1\n",
    "config_dict.log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to save this config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp.save_config(config_dict, name=\"test-config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment logger's `save_config` method automatically encodes our config in JSON and saves the file in our config directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir/test-experiment/config/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"batch_size\": 64,\r\n",
      "    \"batch_size_test\": 1000,\r\n",
      "    \"n_epochs\": 10,\r\n",
      "    \"learning_rate\": 0.01,\r\n",
      "    \"momentum\": 0.9,\r\n",
      "    \"use_cuda\": true,\r\n",
      "    \"rnd_seed\": 1,\r\n",
      "    \"log_interval\": 200\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!tail -n 10 experiment_dir/test-experiment/config/test-config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's continue with our PyTorch workflow - in this case a simple ConvNet trained on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if config_dict.use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('experiment_dir/data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=config_dict.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('experiment_dir/data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=config_dict.batch_size_test, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple cnn model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "if config_dict.use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=config_dict.learning_rate,\n",
    "                     momentum=config_dict.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now during training we'd like to have a simple visualization of the training loss and every `log_interval` batches we also want to save our model.  \n",
    "Our visdom logger's `show_value` simply creates a line plot which is automatically appended with new values every iteration. We could also set an appendix to the environment name here so the plot is displayed in a different environment on the visdom server, but we'll stick to the default here.  \n",
    "Using our experiment logger we can simply save the model providing a name and iteration number. The files are automatically saved in the checkpoint directory of our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if config_dict.use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # plot the training loss\n",
    "        Viz.show_value(loss.data[0], name='Training Loss')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % config_dict.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} samples ({:.0f}%)]\\t Batch Loss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(data), \n",
    "                          len(train_loader.dataset),\n",
    "                          100. * batch_idx / len(train_loader), \n",
    "                          loss.data[0]))\n",
    "            # save the current model weights\n",
    "            Exp.save_model(model, name=\"MNIST_ConvNet\", n_iter=batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every epoch we also want to keep track of our model's performance on the test set. Again we can use `show_value` to display a line plot of the test loss for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if config_dict.use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    # plot the test loss\n",
    "    Viz.show_value(test_loss, name='Test Loss')\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and train our net for the number of epochs saved in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 samples (0%)]\t Batch Loss: 2.314634\n",
      "Train Epoch: 1 [12800/60000 samples (21%)]\t Batch Loss: 0.929577\n",
      "Train Epoch: 1 [25600/60000 samples (43%)]\t Batch Loss: 0.850518\n",
      "Train Epoch: 1 [38400/60000 samples (64%)]\t Batch Loss: 0.311788\n",
      "Train Epoch: 1 [51200/60000 samples (85%)]\t Batch Loss: 0.315522\n",
      "\n",
      "Test set: Average loss: 0.1445, Accuracy: 9553/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 samples (0%)]\t Batch Loss: 0.387098\n",
      "Train Epoch: 2 [12800/60000 samples (21%)]\t Batch Loss: 0.353671\n",
      "Train Epoch: 2 [25600/60000 samples (43%)]\t Batch Loss: 0.523762\n",
      "Train Epoch: 2 [38400/60000 samples (64%)]\t Batch Loss: 0.436220\n",
      "Train Epoch: 2 [51200/60000 samples (85%)]\t Batch Loss: 0.442064\n",
      "\n",
      "Test set: Average loss: 0.1007, Accuracy: 9667/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 samples (0%)]\t Batch Loss: 0.468419\n",
      "Train Epoch: 3 [12800/60000 samples (21%)]\t Batch Loss: 0.355150\n",
      "Train Epoch: 3 [25600/60000 samples (43%)]\t Batch Loss: 0.219614\n",
      "Train Epoch: 3 [38400/60000 samples (64%)]\t Batch Loss: 0.099488\n",
      "Train Epoch: 3 [51200/60000 samples (85%)]\t Batch Loss: 0.157498\n",
      "\n",
      "Test set: Average loss: 0.0750, Accuracy: 9760/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 samples (0%)]\t Batch Loss: 0.256952\n",
      "Train Epoch: 4 [12800/60000 samples (21%)]\t Batch Loss: 0.269517\n",
      "Train Epoch: 4 [25600/60000 samples (43%)]\t Batch Loss: 0.254259\n",
      "Train Epoch: 4 [38400/60000 samples (64%)]\t Batch Loss: 0.255006\n",
      "Train Epoch: 4 [51200/60000 samples (85%)]\t Batch Loss: 0.106559\n",
      "\n",
      "Test set: Average loss: 0.0621, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 samples (0%)]\t Batch Loss: 0.297892\n",
      "Train Epoch: 5 [12800/60000 samples (21%)]\t Batch Loss: 0.241703\n",
      "Train Epoch: 5 [25600/60000 samples (43%)]\t Batch Loss: 0.072185\n",
      "Train Epoch: 5 [38400/60000 samples (64%)]\t Batch Loss: 0.192799\n",
      "Train Epoch: 5 [51200/60000 samples (85%)]\t Batch Loss: 0.190603\n",
      "\n",
      "Test set: Average loss: 0.0653, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 samples (0%)]\t Batch Loss: 0.177426\n",
      "Train Epoch: 6 [12800/60000 samples (21%)]\t Batch Loss: 0.132224\n",
      "Train Epoch: 6 [25600/60000 samples (43%)]\t Batch Loss: 0.082469\n",
      "Train Epoch: 6 [38400/60000 samples (64%)]\t Batch Loss: 0.250655\n",
      "Train Epoch: 6 [51200/60000 samples (85%)]\t Batch Loss: 0.190612\n",
      "\n",
      "Test set: Average loss: 0.0542, Accuracy: 9827/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 samples (0%)]\t Batch Loss: 0.265721\n",
      "Train Epoch: 7 [12800/60000 samples (21%)]\t Batch Loss: 0.050399\n",
      "Train Epoch: 7 [25600/60000 samples (43%)]\t Batch Loss: 0.136675\n",
      "Train Epoch: 7 [38400/60000 samples (64%)]\t Batch Loss: 0.271232\n",
      "Train Epoch: 7 [51200/60000 samples (85%)]\t Batch Loss: 0.092508\n",
      "\n",
      "Test set: Average loss: 0.0568, Accuracy: 9835/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 samples (0%)]\t Batch Loss: 0.118253\n",
      "Train Epoch: 8 [12800/60000 samples (21%)]\t Batch Loss: 0.153760\n",
      "Train Epoch: 8 [25600/60000 samples (43%)]\t Batch Loss: 0.121982\n",
      "Train Epoch: 8 [38400/60000 samples (64%)]\t Batch Loss: 0.025137\n",
      "Train Epoch: 8 [51200/60000 samples (85%)]\t Batch Loss: 0.074096\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 samples (0%)]\t Batch Loss: 0.240387\n",
      "Train Epoch: 9 [12800/60000 samples (21%)]\t Batch Loss: 0.234481\n",
      "Train Epoch: 9 [25600/60000 samples (43%)]\t Batch Loss: 0.252543\n",
      "Train Epoch: 9 [38400/60000 samples (64%)]\t Batch Loss: 0.111205\n",
      "Train Epoch: 9 [51200/60000 samples (85%)]\t Batch Loss: 0.340323\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 samples (0%)]\t Batch Loss: 0.027153\n",
      "Train Epoch: 10 [12800/60000 samples (21%)]\t Batch Loss: 0.133533\n",
      "Train Epoch: 10 [25600/60000 samples (43%)]\t Batch Loss: 0.129283\n",
      "Train Epoch: 10 [38400/60000 samples (64%)]\t Batch Loss: 0.139496\n",
      "Train Epoch: 10 [51200/60000 samples (85%)]\t Batch Loss: 0.119179\n",
      "\n",
      "Test set: Average loss: 0.0446, Accuracy: 9862/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, config_dict.n_epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should also see our saved model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_ConvNet_00000.pth  MNIST_ConvNet_00400.pth  MNIST_ConvNet_00800.pth\r\n",
      "MNIST_ConvNet_00200.pth  MNIST_ConvNet_00600.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_dir/test-experiment/checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! That's it for a very basic use-case, going forward one could e.g. combine the two loggers used in this examples using the `CombinedLogger` class. The benefit of this lies in methods supported by both parts of the combined logger. This way, for example, plots can simultaneously be displayed on the visdom server and saved in the image directory of the experiment - convenient, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
